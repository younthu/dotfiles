# 创建一个工程
scrapy startproject projectname

# 创建spider
scrapy genspider <爬虫名> <domain>

#创建crawler
scrapy genspider -t crawl <爬虫名> <domain>

# 查看工程里面有哪些爬虫
scrapy list

# 查看爬取的页面在浏览器里面的样子
scrapy view https://www.baidu.com

# 判断我们写的parse是否正确
scrapy parse https://www.baidu.com

# 运行爬虫, 需要在爬虫项目下进行
scrapy crawl spidername

# 运行爬虫, 不需要爬虫项目，只要有爬虫定义文件就可以了
scrapy runspider <spider_file.py>

# 在shell里面爬取页面
scrapy shell <page_url>
# UNIX-style
scrapy shell ./path/to/file.html
scrapy shell ../other/path/to/file.html
scrapy shell /absolute/path/to/file.html

# File URI
scrapy shell file:///absolute/path/to/file.html

# shell 里面response获取元素
response.xpath('//*[@id="kkpager"]/div/span/a/@href').extract()
response.xpath('//*[@id="kkpager"]/div/span/a/@href').extract_first()

# 在chrome inspect下测试xpath
$x('//*[@id="kkpager"]/div/span/a/@href')


# 我们可以自定义Item Pipeline，只需要实现指定的方法，其中必须要实现的一个方法是： process_item(item, spider)。
# 另外还有如下几个比较实用的方法。
# open_spider(spider)。
# close_spider(spider)。
# from_crawler(cls, crawler)。
#


# 在运行 crawl 时添加 -a 可以传递Spider参数:
scrapy crawl myspider -a category=electronics
import scrapy

class MySpider(Spider):
    name = 'myspider'

    def __init__(self, category=None, *args, **kwargs):
        super(MySpider, self).__init__(*args, **kwargs)
        self.start_urls = ['http://www.example.com/categories/%s' % category]
        # ...

